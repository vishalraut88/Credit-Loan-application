{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "from hyperopt import hp, tpe, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.preprocessing import preprocessing \n",
    "from utils.utils import feature_importances,initial_hyperparam_search,cast_params_to_proper_types,lgb_f1_score,feature_imp_lgbm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepos= preprocessing()\n",
    "df_credit_application = prepos.read_data(\"credit_applications.csv\")\n",
    "df_customers = prepos.read_data(\"customers.csv\")\n",
    "df_customers.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
    "df_credit_application.drop(\"Unnamed: 0\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete_data = df_customers.merge(df_credit_application,how=\"inner\",on=[\"client_nr\",\"yearmonth\"])\n",
    "print(df_complete_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EDA And feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=df_complete_data[\"credit_application\"])\n",
    "\n",
    "# 0    27971\n",
    "# 1     2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepos.missing_values_intable(df_complete_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams[\"figure.figsize\"] = [5,5]  #set the graph to a smaller size \n",
    "df_complete_data[\"debit_credit_ratio\"]=df_complete_data[\"volume_debit_trx\"]/df_complete_data[\"volume_credit_trx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete_data[\"debit_credit_ratio\"] = np.where(df_complete_data[\"debit_credit_ratio\"]==np.inf,df_complete_data[\"volume_debit_trx\"],df_complete_data[\"debit_credit_ratio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corl= df_complete_data.corr()\n",
    "corl[\"credit_application\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fimp=feature_importances()\n",
    "# X_train_feature_selection, accepted_columns,assoc_result,disallowed_columns = fimp.filter_out_features_based_on_statistical_approach(df_complete_data,[],\"credit_application\")\n",
    "# print(accepted_columns)\n",
    "# print(assoc_result[\"credit_application\"].sort_values(ascending=False))\n",
    "\n",
    "df_complete_data.drop(\"nr_credit_applications\",axis=1,inplace=True)\n",
    "fimp=feature_importances()\n",
    "X_train_feature_selection, accepted_columns,assoc_result,disallowed_columns = fimp.filter_out_features_based_on_statistical_approach(df_complete_data,[],\"credit_application\")\n",
    "print(accepted_columns)\n",
    "print(assoc_result[\"credit_application\"].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a Time being delete rows where CRG is not present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete_data[df_complete_data[\"CRG\"].isna()][\"credit_application\"].value_counts()\n",
    "# 0    5395\n",
    "# 1     142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete_data = df_complete_data[~df_complete_data[\"CRG\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To do : Missing value treatment for CRG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete_data.groupby([\"client_nr\",\"yearmonth\",\"CRG\"])[\"total_nr_trx\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### New features ideas\n",
    "Calculate how many times client has applied for credit in the past for each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nr_credit_in_past(current_index):\n",
    "#     print(current_index)\n",
    "    client_nr = df_complete_data.loc[[current_index]][\"client_nr\"].values[0]\n",
    "    tmp_sum = df_complete_data[(df_complete_data[\"client_nr\"]==client_nr) & (df_complete_data.index <current_index)][\"credit_application\"].sum()\n",
    "    tmp_cnt = df_complete_data[(df_complete_data[\"client_nr\"]==client_nr) & (df_complete_data.index <current_index)][\"credit_application\"].count()\n",
    "    if(tmp_cnt==0):\n",
    "        return 0\n",
    "    else:\n",
    "        return tmp_sum/tmp_cnt\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete_data[\"credit_applied_before_ratio\"]= df_complete_data.index.map(calculate_nr_credit_in_past)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###### Is_debit_more_average\n",
    " \n",
    " Is volume_debit_trx is more than the average of the volume_debit_trx for that client\n",
    " If yes 1 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debit_average_per_client = df_complete_data.groupby([\"client_nr\"]).mean()[\"volume_debit_trx\"].reset_index()\n",
    "# df_complete_data[\"debit_average_per_client\"] = df_complete_data[\"client_nr\"].apply(lambda x: debit_average_per_client[debit_average_per_client[\"client_nr\"]==x][\"volume_debit_trx\"].values[0])\n",
    "# df_complete_data[\"Is_debit_more_average\"]=np.where(df_complete_data[\"debit_average_per_client\"] < df_complete_data[\"volume_debit_trx\"],1,0)\n",
    "# df_complete_data.drop([\"debit_average_per_client\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data strategy\n",
    "\n",
    "As part of train/test data split, 20% of data will be reserved as test data and will not seen by any model.\n",
    "That amounts to approximately = 6 months worth of data.\n",
    "Instead of splitting train/test data randomly, I decided to leave last 6 month data as test data set i.e. March 2016 to August 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_complete_data[df_complete_data[\"yearmonth\"].isin([201608,201607,201606,201605,201604,201603])].copy()\n",
    "y_test = X_test[\"credit_application\"]\n",
    "\n",
    "X_train_org = df_complete_data[~df_complete_data[\"yearmonth\"].isin([201608,201607,201606,201605,201604,201603])].copy()\n",
    "y_train_org = X_train_org[\"credit_application\"]\n",
    "\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train_org,X_train_org[\"credit_application\"],test_size=0.2,random_state=42,stratify=X_train_org[\"credit_application\"])\n",
    "\n",
    "X_test.drop([\"credit_application\"],axis=1,inplace=True)\n",
    "X_train_org.drop([\"credit_application\",\"client_nr\"],axis=1,inplace=True)\n",
    "X_train.drop([\"credit_application\",\"client_nr\"],axis=1,inplace=True)\n",
    "X_val.drop([\"credit_application\",\"client_nr\"],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "# X_test.drop([\"credit_application\",\"client_nr\",\"nr_credit_applications\"],axis=1,inplace=True)\n",
    "# X_train_org.drop([\"credit_application\",\"client_nr\",\"nr_credit_applications\"],axis=1,inplace=True)\n",
    "# X_train.drop([\"credit_application\",\"client_nr\",\"nr_credit_applications\"],axis=1,inplace=True)\n",
    "# X_val.drop([\"credit_application\",\"client_nr\",\"nr_credit_applications\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Using Lightgbm feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import plot_importance\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params_scope = {\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.001, 0.1),\n",
    "    'num_leaves': hp.quniform('num_leaves', 2, 15, 1),\n",
    "    'n_estimators': hp.quniform('n_estimators', 10, 500, 10),\n",
    "    'scale_pos_weight': hp.quniform('scale_pos_weight', 5, 100, 5),\n",
    "    'reg_lambda ': hp.uniform('reg_lambda', 10.00, 100.0),\n",
    "    'pos_bagging_fraction': hp.uniform('pos_bagging_fraction', 0.0, 1.0),\n",
    "    'max_bin': hp.quniform('max_bin', 16, 256, 16)\n",
    "                            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_scope = {\n",
    "# #                 'learning_rate': [x/1000 for x in range(1, 110,10)],\n",
    "#                 'n_estimators': [x for x in range(10, 500,50)],\n",
    "#                'num_leaves': [x for x in range(2, 15,2)],\n",
    "#                'scale_pos_weight':[x for x in range(85, 100,5)],\n",
    "#                'objective': ['binary'],\n",
    "# #                'reg_lambda ': [x for x in range(10, 100,10)],\n",
    "#                }\n",
    "\n",
    "# import lightgbm as lgb\n",
    "# model = lgb.LGBMClassifier(**params_scope)\n",
    "# grid = GridSearchCV(model, param_grid=params_scope, verbose=1, cv=3, n_jobs=-1)\n",
    "# # Run the grid\n",
    "# grid.fit(X_train,y_train)\n",
    "\n",
    "# # Print the best parameters found\n",
    "# print(grid.best_params_)\n",
    "# print(grid.best_score_)\n",
    "# model = grid.best_estimator_\n",
    "# feature_imp_df = pd.DataFrame(data={'feature_name':model.feature_name_,\n",
    "#                    'feature_importance':model.feature_importances_},\n",
    "            \n",
    "#             )\n",
    "# feature_imp_df.sort_values(by=\"feature_importance\",ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,initial_params = feature_imp_lgbm(X_train,y_train,X_val,y_val,params_scope)\n",
    "feature_imp_df = pd.DataFrame(data={'feature_name':model.feature_name_,\n",
    "                   'feature_importance':model.feature_importances_},\n",
    "            \n",
    "            )\n",
    "feature_imp_df.sort_values(by=\"feature_importance\",ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(initial_params)\n",
    "feature_imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_val[X_train.columns])\n",
    "shap.summary_plot(shap_values, X_val[X_train.columns], plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.metrics import roc_auc_score,f1_score,confusion_matrix,precision_score,recall_score,roc_curve\n",
    "\n",
    "# kfolds = StratifiedKFold(5)\n",
    "# current_f1_scores=[]\n",
    "# roc_auc_scores_val=[]\n",
    "# precision_score_val=[]\n",
    "# recall_score_val=[]\n",
    "# fpr_scores=[]\n",
    "# tpr_scores=[]\n",
    "# mean_roc_auc_scores_val = []\n",
    "# mean_precision_scores_val = []\n",
    "# mean_recall_scores_val = []\n",
    "# mean_f1_scores_val = []\n",
    "\n",
    "# for train_idx ,val_index in kfolds.split(X_train_org,y_train_org):\n",
    "#     model.fit(\n",
    "#                 X_train_org.iloc[train_idx],\n",
    "#                 y_train_org.iloc[train_idx],\n",
    "#                eval_metric=lgb_f1_score\n",
    "#             )\n",
    "#     y_pred=np.where(model.predict_proba(X_train_org.iloc[val_index])[:,1] >0.4,1,0)\n",
    "#     current_f1_scores.append(f1_score(y_train_org.iloc[val_index], y_pred))\n",
    "#     tn, fp, fn, tp = confusion_matrix(y_train_org.iloc[val_index],y_pred).ravel()\n",
    "#     print(confusion_matrix(y_train_org.iloc[val_index],y_pred))\n",
    "#     roc_auc_scores_val.append(roc_auc_score(y_true=y_train_org.iloc[val_index], y_score=y_pred))\n",
    "#     precision_score_val.append(precision_score(y_true=y_train_org.iloc[val_index],y_pred=y_pred))\n",
    "#     recall_score_val.append(recall_score(y_true=y_train_org.iloc[val_index],y_pred=y_pred))\n",
    "#     fpr_val, tpr_val, _ = roc_curve(y_true=y_train_org.iloc[val_index], y_score=y_pred)\n",
    "#     fpr_scores.append(fpr_val)\n",
    "#     tpr_scores.append(tpr_val)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score,f1_score,confusion_matrix,precision_score,recall_score,roc_curve\n",
    "\n",
    "def cross_validation(model,X_train_org,y_train_org,threshold):\n",
    "    \n",
    "    kfolds = StratifiedKFold(5)\n",
    "    current_f1_scores=[]\n",
    "    roc_auc_scores_val=[]\n",
    "    precision_score_val=[]\n",
    "    recall_score_val=[]\n",
    "    fpr_scores=[]\n",
    "    tpr_scores=[]\n",
    "\n",
    "    \n",
    "    \n",
    "    for train_idx ,val_index in kfolds.split(X_train_org,y_train_org):\n",
    "        model.fit(\n",
    "                    X_train_org.iloc[train_idx],\n",
    "                    y_train_org.iloc[train_idx]\n",
    "#                   ,eval_metric=lgb_f1_score\n",
    "                )\n",
    "        y_pred=np.where(model.predict_proba(X_train_org.iloc[val_index])[:,1] >threshold,1,0)\n",
    "        current_f1_scores.append(f1_score(y_train_org.iloc[val_index], y_pred))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_train_org.iloc[val_index],y_pred).ravel()\n",
    "        print(confusion_matrix(y_train_org.iloc[val_index],y_pred))\n",
    "        print(y_pred)\n",
    "        roc_auc_scores_val.append(roc_auc_score(y_train_org.iloc[val_index], y_pred))\n",
    "        precision_score_val.append(precision_score(y_true=y_train_org.iloc[val_index],y_pred=y_pred))\n",
    "        recall_score_val.append(recall_score(y_true=y_train_org.iloc[val_index],y_pred=y_pred))\n",
    "        fpr_val, tpr_val, _ = roc_curve(y_true=y_train_org.iloc[val_index], y_score=y_pred)\n",
    "        fpr_scores.append(fpr_val)\n",
    "        tpr_scores.append(tpr_val)\n",
    "    \n",
    "    return current_f1_scores,roc_auc_scores_val,precision_score_val,recall_score_val,fpr_scores,tpr_scores\n",
    "\n",
    "    \n",
    "\n",
    "def plot_roc(fpr,tpr,roc_auc,color_ip):\n",
    "    print('ROC AUC=%0.2f'%roc_auc)\n",
    "    plt.plot(fpr,tpr,label='AUC=%0.2f'%roc_auc,color=color_ip)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1],[0,1],'b--')\n",
    "    plt.xlim([0,1])\n",
    "    plt.ylim([0,1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.grid(True) \n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_f1_scores,roc_auc_scores_val,precision_score_val,recall_score_val,fpr_scores,tpr_scores=cross_validation(model,X_train_org,y_train_org,0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list=[\"green\",\"red\",\"black\",\"yellow\",\"orange\"]\n",
    "for fpr_val,tpr_val,roc_auc_score,color in zip(fpr_scores,tpr_scores,roc_auc_scores_val,color_list):\n",
    "    plot_roc(fpr_val,tpr_val,roc_auc_score,color)\n",
    "    \n",
    "    \n",
    "print('Mean ROC AUC score on validation data =%0.2f'%np.mean(roc_auc_scores_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(f1_scores,recall_scores,precision_scores,dataset=\"validation\"):\n",
    "    print(\"F1 scores on \"+ dataset+\" data:\")\n",
    "    print(f1_scores)\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"Recall scores on \"+ dataset+\" data:\")\n",
    "    print(recall_scores)\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"Mean f1 score on \"+ dataset+\" data:\")\n",
    "    print(np.mean(f1_scores))\n",
    "\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"Mean recall score on \"+ dataset+\" data:\")\n",
    "    print(np.mean(recall_scores))\n",
    "\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"Mean precision score on \"+ dataset+\" data:\")\n",
    "    print(np.mean(precision_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(current_f1_scores,recall_score_val,precision_score_val,\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### On test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test_pred = model.predict(X_test)\n",
    "for i in np.arange(0.3,0.65,0.051):\n",
    "    i=(round(i,2))\n",
    "    print(\"---------------THRESHOLD =\"+str(i)+\" ----------------------\")\n",
    "\n",
    "    y_test_pred=np.where(model.predict_proba(X_test[X_train.columns])[:,1] >i,1,0)\n",
    "\n",
    "    print(confusion_matrix(y_test,y_test_pred))\n",
    "    print(\"F1 score on Test data:\")\n",
    "    print(f1_score(y_test,y_test_pred))\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"Recall score on Test data:\")\n",
    "    print(recall_score(y_test,y_test_pred))\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"Precision score on Test data:\")\n",
    "    print(precision_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test=X_test[X_train.columns]\n",
    "y_test_pred=np.where(model.predict_proba(X_test[X_train.columns])[:,1] >0.4,1,0)\n",
    "print(confusion_matrix(y_test,y_test_pred))\n",
    "print(\"F1 score on Test data:\")\n",
    "print(f1_score(y_test,y_test_pred))\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"Recall score on Test data:\")\n",
    "print(recall_score(y_test,y_test_pred))\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"Precision score on Test data:\")\n",
    "print(precision_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save preds to csv file\n",
    "\n",
    "X_test[\"ground_truth\"] = y_test\n",
    "X_test[\"pred_lgbm\"]=y_test_pred\n",
    "# X_test.to_csv(\"predictions_on_test_set_lgbm.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std=StandardScaler()\n",
    "Xformed_data = std.fit_transform(X_train_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xformed_data = pd.DataFrame(Xformed_data,columns=X_train_org.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "def svm_hyper(X_train_transformed, y_train):\n",
    "    svc = SVC()\n",
    "    C_range = [0.001,0.1,1,10]\n",
    "#     gamma_range = [0.001,0.1,1,'scale', 'auto']    \n",
    "    param_grid = { \n",
    "        \"C\": C_range,\n",
    "        \"kernel\": ['rbf', 'poly']\n",
    "#         ,\"gamma\": gamma_range\n",
    "        }\n",
    "    print(param_grid)\n",
    "    scoring = ['f1']\n",
    "#     kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "    random_search = RandomizedSearchCV(estimator=svc, \n",
    "                               param_distributions=param_grid, \n",
    "                               n_iter=5,\n",
    "                               scoring=scoring,\n",
    "                               refit=\"f1\",        \n",
    "                               n_jobs=-1, \n",
    "                               cv=3)\n",
    "    # Fit grid search\n",
    "    random_result = random_search.fit(X_train_transformed, y_train)\n",
    "    # Print grid search summary\n",
    "    model = random_search.best_estimator_\n",
    "    return model,random_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,random_search = svm_hyper(Xformed_data,y_train_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# pickle.dump(model,open(\"output files/model_svm.pkl\",\"wb\"))\n",
    "model_svm = pickle.load(open(\"output files/model_svm.pkl\",\"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model_svm.predict(X_test[X_train.columns])\n",
    "print(confusion_matrix(y_test_pred,y_test))\n",
    "print(f1_score(y_test_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score,RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n",
    "\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4,10]\n",
    "bootstrap = [True, False]\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "random_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    model_rf = BalancedRandomForestClassifier()\n",
    "    rf_random = RandomizedSearchCV(estimator = model_rf, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "    rf_random.fit(X_train_org, y_train_org)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf=rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "        \n",
    "    current_f1_scores,roc_auc_scores_val,precision_score_val,recall_score_val,fpr_scores,tpr_scores=cross_validation(model_rf,X_train_org,y_train_org,0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list=[\"green\",\"red\",\"black\",\"yellow\",\"orange\"]\n",
    "for fpr_val,tpr_val,roc_auc_score,color in zip(fpr_scores,tpr_scores,roc_auc_scores_val,color_list):\n",
    "    plot_roc(fpr_val,tpr_val,roc_auc_score,color)\n",
    "    \n",
    "    \n",
    "print('Mean ROC AUC score on validation data =%0.2f'%np.mean(roc_auc_scores_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(current_f1_scores,recall_score_val,precision_score_val,\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test_pred = model.predict(X_test)\n",
    "for i in np.arange(0.3,0.65,0.051):\n",
    "    i=(round(i,2))\n",
    "    print(\"---------------THRESHOLD =\"+str(i)+\" ----------------------\")\n",
    "\n",
    "    y_test_pred=np.where(model_rf.predict_proba(X_test[X_train.columns])[:,1] >i,1,0)\n",
    "\n",
    "    print(confusion_matrix(y_test,y_test_pred))\n",
    "    print(\"F1 score on Test data:\")\n",
    "    print(f1_score(y_test,y_test_pred))\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"Recall score on Test data:\")\n",
    "    print(recall_score(y_test,y_test_pred))\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"Precision score on Test data:\")\n",
    "    print(precision_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test_pred=np.where(model_rf.predict_proba(X_test[X_train.columns])[:,1] >0.5,1,0)\n",
    "print(confusion_matrix(y_test,y_test_pred))\n",
    "print(\"F1 score on Test data:\")\n",
    "print(f1_score(y_test,y_test_pred))\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"Recall score on Test data:\")\n",
    "print(recall_score(y_test,y_test_pred))\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"Precision score on Test data:\")\n",
    "print(precision_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save preds to csv file\n",
    "\n",
    "# X_test[\"ground_truth\"] = y_test\n",
    "X_test[\"pred_rf\"]=y_test_pred\n",
    "X_test.to_csv(\"output files/predictions_on_test_set_rf_threshold_0.5.csv\")\n",
    "\n",
    "\n",
    "pickle.dump(model,open(\"output files/model_rf.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unsupervised (Isolation forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.metrics import f1_score,roc_auc_score,confusion_matrix,recall_score,precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_iso = IsolationForest(contamination=0.03, max_features=0.1, max_samples=0.1,n_estimators=230)\n",
    "model_iso.fit(X_train_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_iso.predict(X_test[X_train.columns])\n",
    "y_test_pred = model_iso.predict(X_test[X_train.columns])\n",
    "y_test_pred = [1 if x==-1 else 0 for x in y_test_pred ]\n",
    "score = f1_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)\n",
    "print(recall_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[\"iso_pred\"]=y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[\"final_pred\"]=np.where(X_test[\"pred_lgbm\"]+X_test[\"pred_rf\"]+X_test[\"iso_pred\"]>1,1,0)\n",
    "confusion_matrix(y_test,X_test[\"final_pred\"])\n",
    "recall_score(y_test,X_test[\"final_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(X_test[\"ground_truth\"],X_test[\"final_pred\"]))\n",
    "print(recall_score(X_test[\"ground_truth\"],X_test[\"final_pred\"]))\n",
    "print(precision_score(X_test[\"ground_truth\"],X_test[\"final_pred\"]))\n",
    "confusion_matrix(X_test[\"ground_truth\"],X_test[\"final_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(X_test[\"ground_truth\"],X_test[\"pred_lgbm\"]))\n",
    "print(recall_score(X_test[\"ground_truth\"],X_test[\"pred_lgbm\"]))\n",
    "print(precision_score(X_test[\"ground_truth\"],X_test[\"pred_lgbm\"]))\n",
    "confusion_matrix(X_test[\"ground_truth\"],X_test[\"pred_lgbm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(\"output files/ensemble_voting.csv\")\n",
    "pickle.dump(model_iso,open('output files/model_iso.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_tasks",
   "language": "python",
   "name": "nlp_tasks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
